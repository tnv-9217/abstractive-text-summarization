{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c3dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fe388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7ba7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e9706b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc476c7ea8b48c783bcbe1a40693270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnv18/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c1eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Image captioning is a fast-growing research field of computer vision and natural language processing that involves creating text explanations for images. This study aims to develop a system that uses a pre-trained convolutional neural network (CNN) to extract features from an image, integrates the features with an attention mechanism, and creates captions using a recurrent neural network (RNN). To encode an image into a feature vector as graphical attributes, we employed multiple pre-trained convolutional neural networks. Following that, a language model known as GRU was chosen as the decoder to construct the descriptive sentence. In order to increase performance, we merged the Bahdanau attention model with GRU to allow learning to be focused on a specific portion of the image. On the MSCOCO dataset, the experimental results achieve competitive performance against state-of-the-art approaches.\n",
    "Keywords: Image captioning; Attention mechanism; Inception V3; Convolutional Neural Network; GRU. 1. Introduction\n",
    "Image caption generation is a challenging artificial intelligence task in which a textual explanation for a given image must be generated. It needs both computer vision approaches to understand the image's content and a language model from the field of natural language processing to convert the image's understanding into words in the right sequence. with the development of deep learning in computer vision, the twenty- first century, or the Artificial Intelligence Era, began. The main aim of the data scientists was to extract visual features from images, identify them, and detect objects. in recent years, Natural Language Processing algorithms have played an increasingly important role in deep learning research, allowing computers to learn from the world of words, interpret them, classify them, and extract features. Natural language processing (NLP), on the other hand, has spawned a slew of applications ranging from basic text classification to fully automated natural language chatbots. In the Deep Learning domain, it has been a critical and fundamental challenge. Captioning images has a wide range of applications, including (i) transcribing scenes for people who are blind [1][2], (ii) classifying videos and photographs based on various situations [3], (iii) image-based search engines for better results, [4] (iv) visual question answering [5], and (v) context comprehension [6].\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2faac376",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = text.strip().replace('\\n','')\n",
    "t5_input_text = 'summarize: ' + preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cbcf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"summarize: Image captioning is a fast-growing research field of computer vision and natural language processing that involves creating text explanations for images. This study aims to develop a system that uses a pre-trained convolutional neural network (CNN) to extract features from an image, integrates the features with an attention mechanism, and creates captions using a recurrent neural network (RNN). To encode an image into a feature vector as graphical attributes, we employed multiple pre-trained convolutional neural networks. Following that, a language model known as GRU was chosen as the decoder to construct the descriptive sentence. In order to increase performance, we merged the Bahdanau attention model with GRU to allow learning to be focused on a specific portion of the image. On the MSCOCO dataset, the experimental results achieve competitive performance against state-of-the-art approaches.Keywords: Image captioning; Attention mechanism; Inception V3; Convolutional Neural Network; GRU. 1. IntroductionImage caption generation is a challenging artificial intelligence task in which a textual explanation for a given image must be generated. It needs both computer vision approaches to understand the image's content and a language model from the field of natural language processing to convert the image's understanding into words in the right sequence. with the development of deep learning in computer vision, the twenty- first century, or the Artificial Intelligence Era, began. The main aim of the data scientists was to extract visual features from images, identify them, and detect objects. in recent years, Natural Language Processing algorithms have played an increasingly important role in deep learning research, allowing computers to learn from the world of words, interpret them, classify them, and extract features. Natural language processing (NLP), on the other hand, has spawned a slew of applications ranging from basic text classification to fully automated natural language chatbots. In the Deep Learning domain, it has been a critical and fundamental challenge. Captioning images has a wide range of applications, including (i) transcribing scenes for people who are blind [1][2], (ii) classifying videos and photographs based on various situations [3], (iii) image-based search engines for better results, [4] (iv) visual question answering [5], and (v) context comprehension [6].\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4194d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t5_input_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ad651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.encode(t5_input_text, return_tensors='pt', max_length = 512).to(device)\n",
    "truncation=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e77ea809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(tokenized_text, min_length=30, max_length=120)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f767ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image captioning is a fast-growing research field of computer vision and natural language processing. the research aims to develop a system that uses a pre-trained convolutional neural network (CNN) to extract features from an image, integrates the features with an attention mechanism, and creates captions using a recurrent neural network (RNN)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecced57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
